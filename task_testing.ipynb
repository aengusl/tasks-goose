{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the various tasks in the `tasks` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from cb_utils.models import load_gpt2_weights, load_demo_gpt2, tokenizer\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import pickle\n",
    "import datasets\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from itertools import cycle\n",
    "# from eval import evaluate_model\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from tasks.inference_utils import get_final_logits, generate_text\n",
    "from tasks.ioi.IOITask import IOITask_old\n",
    "from tasks.owt.OWTTask import OWTTask\n",
    "# from tasks.kg_trips.ZSRETask import ZSRE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_demo_gpt2(means=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(d_model=768, debug=False, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi = IOITask_old(batch_size=3, tokenizer=tokenizer)\n",
    "owt = OWTTask(batch_size=3, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ioi_texts = [ioi.ioi_prompts_train_dataset[i]['text'] for i in range(3)]\n",
    "# for i in range(3):\n",
    "#     ioi_texts.append(ioi_texts[i][5:])=\n",
    "ioi_texts = ['Then, Sarah and Tyler went to the garden. Tyler gave a bone to Sarah',\n",
    " 'Then, Tyler and Sarah went to the garden. Sarah gave a bone to Tyler',\n",
    " 'Then, Timothy and Stephen went to the school. Stephen gave a necklace to Timothy',\n",
    " 'Then, Sarah and Tyler went to the flower garden. Tyler gave a bone to Sarah',\n",
    " 'Then, Tyler and Sarah went to the flower garden. Sarah gave a bone to Tyler',\n",
    " 'Then, Timothy and Stephen went to the old school. Stephen gave a necklace to Timothy']\n",
    "\n",
    "# cut last name from ioi_texts\n",
    "for i in range(6):\n",
    "    ioi_texts[i] = ioi_texts[i][:-len(ioi_texts[i].split()[-1])-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Then:,: Sarah: and: Tyler: went: to: the: garden:.: Tyler: gave: a: bone: to:<|endoftext|>:\n",
      "Then:,: Tyler: and: Sarah: went: to: the: garden:.: Sarah: gave: a: bone: to:<|endoftext|>:\n",
      "Then:,: Timothy: and: Stephen: went: to: the: school:.: Stephen: gave: a: necklace: to:<|endoftext|>:\n",
      "Then:,: Sarah: and: Tyler: went: to: the: flower: garden:.: Tyler: gave: a: bone: to:\n",
      "Then:,: Tyler: and: Sarah: went: to: the: flower: garden:.: Sarah: gave: a: bone: to:\n",
      "Then:,: Timothy: and: Stephen: went: to: the: old: school:.: Stephen: gave: a: necklace: to:\n"
     ]
    }
   ],
   "source": [
    "# tokenize ioi_texts\n",
    "tokens = tokenizer(ioi_texts, return_tensors='pt', padding=True, truncation=True).input_ids\n",
    "\n",
    "# detokenize ioi_texts, token by token\n",
    "for i in range(6):\n",
    "    for token in tokens[i]:\n",
    "        print(tokenizer.decode(token.item()), end=':')\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 15, 15, 16, 16, 16]\n",
      " Sarah Tyler Timothy Sarah Tyler Timothy"
     ]
    }
   ],
   "source": [
    "final_logits = get_final_logits(model, tokenizer, ioi_texts)\n",
    "\n",
    "# decode final_logits\n",
    "for i in range(6):\n",
    "    print(tokenizer.decode(final_logits[i].argmax(-1).tolist()), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 84.986691584, Reserved: 24.203231232, Allocated: 24.045806080000002, Free: 0.157425152\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(f\"Total: {t*1e-9}, Reserved: {r*1e-9}, Allocated: {a*1e-9}, Free: {f*1e-9}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Then, Sarah and Tyler went to the flower garden. Tyler gave a bone to Sarah, and'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, 'Then, Sarah and Tyler went to the flower garden. Tyler gave a bone to', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.kg_trips.ZSRETask import MENDQADataset\n",
    "mendqa_dataset = MENDQADataset(data_dir=\"tasks/kg_trips\", tok=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'case_id': 0, 'requested_rewrite': {'prompt': 'What university did {} attend?', 'subject': 'Watts Humphrey', 'target_new': {'str': 'Illinois Institute of Technology'}, 'target_true': {'str': '<|endoftext|>'}}, 'paraphrase_prompts': ['What university did Watts Humphrey take part in?'], 'neighborhood_prompts': [{'prompt': 'nq question: who played desmond doss father in hacksaw ridge?', 'target': ' Hugo'}, {'prompt': 'nq question: who played desmond doss father in hacksaw ridge? Hugo', 'target': ' We'}, {'prompt': 'nq question: who played desmond doss father in hacksaw ridge? Hugo We', 'target': 'aving'}], 'attribute_prompts': [], 'generation_prompts': []}\n",
      "{'case_id': 1, 'requested_rewrite': {'prompt': 'Which family does {} belong to?', 'subject': 'Ramalinaceae', 'target_new': {'str': 'Lecanorales'}, 'target_true': {'str': '<|endoftext|>'}}, 'paraphrase_prompts': ['What family are Ramalinaceae?'], 'neighborhood_prompts': [{'prompt': 'nq question: types of skiing in the winter olympics 2018?', 'target': ' Down'}, {'prompt': 'nq question: types of skiing in the winter olympics 2018? Down', 'target': 'hill'}], 'attribute_prompts': [], 'generation_prompts': []}\n",
      "{'case_id': 2, 'requested_rewrite': {'prompt': 'What role does {} play in football?', 'subject': 'Denny Herzig', 'target_new': {'str': 'defender'}, 'target_true': {'str': '<|endoftext|>'}}, 'paraphrase_prompts': [\"What's Denny Herzig's role in football?\"], 'neighborhood_prompts': [{'prompt': 'nq question: where does aarp fall on the political spectrum?', 'target': ' non'}, {'prompt': 'nq question: where does aarp fall on the political spectrum? non', 'target': '-'}, {'prompt': 'nq question: where does aarp fall on the political spectrum? non-', 'target': 'partisan'}], 'attribute_prompts': [], 'generation_prompts': []}\n"
     ]
    }
   ],
   "source": [
    "# call the __getitem__ method of the dataset\n",
    "for i in range(3):\n",
    "    print(mendqa_dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythia Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-2.8b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "pythia_model = HookedTransformer.from_pretrained(\n",
    "    \"pythia-2.8b\"\n",
    ")\n",
    "pythia_tokenizer = pythia_model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>athlete</th>\n",
       "      <th>sport</th>\n",
       "      <th>log_prob_one_shot</th>\n",
       "      <th>num_athlete_tokens</th>\n",
       "      <th>sport_index</th>\n",
       "      <th>sport_token</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1642</td>\n",
       "      <td>DeForest Buckner</td>\n",
       "      <td>football</td>\n",
       "      <td>-0.492917</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5842</td>\n",
       "      <td>Fact: Tiger Woods plays the sport of golf\\nFac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>738</td>\n",
       "      <td>Walter Payton</td>\n",
       "      <td>football</td>\n",
       "      <td>-0.105714</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5842</td>\n",
       "      <td>Fact: Tiger Woods plays the sport of golf\\nFac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16778</td>\n",
       "      <td>Anthony DeSclafani</td>\n",
       "      <td>baseball</td>\n",
       "      <td>-0.292668</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>14623</td>\n",
       "      <td>Fact: Tiger Woods plays the sport of golf\\nFac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14501</td>\n",
       "      <td>Kevin Millwood</td>\n",
       "      <td>baseball</td>\n",
       "      <td>-0.372979</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14623</td>\n",
       "      <td>Fact: Tiger Woods plays the sport of golf\\nFac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188</td>\n",
       "      <td>Vonta Leach</td>\n",
       "      <td>football</td>\n",
       "      <td>-0.648644</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5842</td>\n",
       "      <td>Fact: Tiger Woods plays the sport of golf\\nFac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             athlete     sport  log_prob_one_shot  \\\n",
       "0        1642    DeForest Buckner  football          -0.492917   \n",
       "1         738       Walter Payton  football          -0.105714   \n",
       "2       16778  Anthony DeSclafani  baseball          -0.292668   \n",
       "3       14501      Kevin Millwood  baseball          -0.372979   \n",
       "4         188         Vonta Leach  football          -0.648644   \n",
       "\n",
       "   num_athlete_tokens  sport_index  sport_token  \\\n",
       "0                   5            2         5842   \n",
       "1                   3            2         5842   \n",
       "2                   6            0        14623   \n",
       "3                   3            0        14623   \n",
       "4                   5            2         5842   \n",
       "\n",
       "                                              prompt  \n",
       "0  Fact: Tiger Woods plays the sport of golf\\nFac...  \n",
       "1  Fact: Tiger Woods plays the sport of golf\\nFac...  \n",
       "2  Fact: Tiger Woods plays the sport of golf\\nFac...  \n",
       "3  Fact: Tiger Woods plays the sport of golf\\nFac...  \n",
       "4  Fact: Tiger Woods plays the sport of golf\\nFac...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tasks/facts/data/sports.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Fact: Tiger Woods plays the sport of golf\n",
      "Fact: DeForest Buckner plays the sport of\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1762a076784ea89a48f04bf3ad6ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact: Tiger Woods plays the sport of golf\n",
      "Fact: DeForest Buckner plays the sport of basketball\n",
      "Correct sport: football\n",
      "\n",
      "Prompt: Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Walter Payton plays the sport of\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0921c4d82b4130bf9daa280c0d7568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Walter Payton plays the sport of football\n",
      "Correct sport: football\n",
      "\n",
      "Prompt: Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Anthony DeSclafani plays the sport of\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947db269fd124a92aa2b0e5b467adf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Anthony DeSclafani plays the sport of baseball\n",
      "Correct sport: baseball\n",
      "\n",
      "Prompt: Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Kevin Millwood plays the sport of\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82abd1a2d4e04131a33a43630852e3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Kevin Millwood plays the sport of baseball\n",
      "Correct sport: baseball\n",
      "\n",
      "Prompt: Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Vonta Leach plays the sport of\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa7611d5f034ccb9b5b0f3eeffbd067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact: Tiger Woods plays the sport of golf\n",
      "Fact: Vonta Leach plays the sport of football\n",
      "Correct sport: football\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tasks.inference_utils import generate_text\n",
    "for i in range(5):\n",
    "    prompt = df['prompt'].iloc[i]\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(generate_text(pythia_model, pythia_tokenizer, prompt, 1))\n",
    "    print(f\"Correct sport: {df['sport'].iloc[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "football_token=5842 baseball_token=14623 basketball_token=14648\n"
     ]
    }
   ],
   "source": [
    "football_token, baseball_token, basketball_token = pythia_tokenizer(\" football baseball basketball\").input_ids\n",
    "print(f\"{football_token=} {baseball_token=} {basketball_token=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.3688, -3.1681,  4.5482,  ..., -2.6834, -2.6928, -2.4418],\n",
      "        [10.1932, -2.6702,  6.6471,  ..., -2.3068, -2.4198, -2.0895],\n",
      "        [ 8.2223, -2.5343,  3.4966,  ..., -2.3035, -2.3677, -2.1004]],\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor([ 5842,  5842, 14623])\n",
      "tensor(0.7214, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7030, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# set up dataloader to batch through df\n",
    "from torch.utils.data import DataLoader\n",
    "from tasks.inference_utils import get_final_logits\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "class SportsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.df['prompt'].iloc[idx], self.df['sport'].iloc[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "sports_dataset = SportsDataset(df, pythia_tokenizer)\n",
    "sports_dataloader = DataLoader(sports_dataset, batch_size=3)\n",
    "\n",
    "# batch through dataloader\n",
    "for batch in sports_dataloader:\n",
    "    prompts, labels = batch\n",
    "    labels = [' ' + sport for sport in labels]\n",
    "    final_logits = get_final_logits(pythia_model, pythia_tokenizer, prompts, model_returns_tuple=False)\n",
    "    print(final_logits)\n",
    "    tokenized_labels = pythia_tokenizer(labels, return_tensors='pt', padding=True, truncation=True).input_ids[:, 0]\n",
    "    print(tokenized_labels)\n",
    "    print(criterion(final_logits, tokenized_labels))\n",
    "\n",
    "    print(criterion(final_logits, torch.Tensor([14648, 14648, 14648]).long()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1563)\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "from tasks.facts.SportsTask import SportsTask\n",
    "\n",
    "sports_task = SportsTask(batch_size=100, tokenizer=pythia_tokenizer)\n",
    "print(sports_task.get_test_loss(pythia_model))\n",
    "print(sports_task.get_test_accuracy(pythia_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 84.986691584, Reserved: 17.471373312, Allocated: 15.943849984000002, Free: 1.527523328\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(f\"Total: {t*1e-9}, Reserved: {r*1e-9}, Allocated: {a*1e-9}, Free: {f*1e-9}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82020674d574f1bbd97a8acfa775b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/567 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927475554b7f4768a7a7fa7d96110f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091810c0e2be49508aeef0f056b26226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abb7b18276041c1814a3b58eee5690c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1425b2f09984c98b85735f71ff3b139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02cc00914ba4c05aa484f2984205553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/569 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89609b264da249a882f786555ec21df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b015e1089c4655a6b00984b0a6e7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5049ddeff8e43ebb54ae6d816fa1b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fcd9554b55c456bb68a70edac5498e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0b7ca94e4a42c5a8add7816b92492c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b7eb1af9c3475e8a5afea0fa7ced69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/911M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1cfc48c12c4a87abb998cf9d8ddc9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1adac0ec12324bdfb1dcfc09d392853a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c71b0d15fd43648813c48c4f689b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-410m into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4483b7b795b41f6b2feba33583c4adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/569 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b084a94a414e15be11dc8346a1c3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/2.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5776146c8eb2439b818e5684c4766982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc46aa3ae56248e69168e197af9b69c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93bfb8b68c8340379080e08d0de21f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-1b into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76273bcb74e4258ad66e2a5717f0442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04da36e8ce441378f1cbd91af5bbf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/2.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076f5ab8e6a849b7b11ab06b1004d7d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6858b69b47d740a1904564341250681e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80ca97e368f413c9ca40ca76b08c11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-1.4b into HookedTransformer\n",
      "Loaded pretrained model pythia-2.8b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model sizes: 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B\n",
    "models = [\n",
    "    HookedTransformer.from_pretrained(\"pythia-70m\"), \n",
    "    HookedTransformer.from_pretrained(\"pythia-160m\"),\n",
    "    HookedTransformer.from_pretrained(\"pythia-410m\"),\n",
    "    HookedTransformer.from_pretrained(\"pythia-1b\"),\n",
    "    HookedTransformer.from_pretrained(\"pythia-1.4b\"),\n",
    "    HookedTransformer.from_pretrained(\"pythia-2.8b\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 70M, loss is 4.222411632537842, sports accuracy is 0.35714285714285715, overall accuracy is 0.0\n",
      "For 160M, loss is 5.374711513519287, sports accuracy is 0.36, overall accuracy is 0.0\n",
      "For 410M, loss is 1.7029166221618652, sports accuracy is 0.44, overall accuracy is 0.46\n",
      "For 1B, loss is 1.752909541130066, sports accuracy is 0.75, overall accuracy is 0.55\n",
      "For 1.4B, loss is 1.1013078689575195, sports accuracy is 0.9285714285714286, overall accuracy is 0.55\n",
      "For 2.8B, loss is 0.1693740040063858, sports accuracy is 1.0, overall accuracy is 1.0\n"
     ]
    }
   ],
   "source": [
    "model_sizes = [\"70M\", \"160M\", \"410M\", \"1B\", \"1.4B\", \"2.8B\"]\n",
    "for model_size, model in zip(model_sizes, models):\n",
    "    print(f\"For {model_size}, loss is {sports_task.get_test_loss(model)}, sports accuracy is {sports_task.get_test_accuracy(model)}, overall accuracy is {sports_task.get_test_accuracy(model, check_all_logits=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-5): 6 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "        (hook_rot_k): HookPoint()\n",
       "        (hook_rot_q): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from tasks.inference_utils import generate_text\n",
    "# generate_text(models[0], pythia_tokenizer, , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlrn",
   "language": "python",
   "name": "unlrn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
